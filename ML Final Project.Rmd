---
title: "ML Final Project"
author: "Miriam Ben-Hamo"
date: "December 25, 2017"
output: html_document
---

# Excutive Summary 
The goal of this project is to correctly predict how well a person performed barbell lift. The level of performance is categorized from A to E, where A is a perfect barbell lift and the other categories denote different common mistakes in barbell lifting. The data itself was downloaded from [this site](http://groupware.les.inf.puc-rio.br/har). Below, I detail the steps of analysis from fetching the data, cleaning and organizing the data, and to building a model, selecting best model and predicting.

# Fetching the data
```{r include=FALSE,warning=FALSE,message=FALSE}
setwd("D:/Data Science/Course 8/ML-Final-Project")
library(dplyr)
library(ggplot2)
library(caret)
library(rattle)
```


```{r warning=FALSE,message=FALSE}
trainurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

dir <- paste(getwd(),"/training.csv",sep="")
if(!file.exists(dir)){
    download.file(trainurl,destfile = dir)
    }
train <- read.csv("training.csv",na.strings = c("","#DIV/0!"))

dir <- paste(getwd(),"/testing.csv",sep="")
if(!file.exists(dir)){
    download.file(testurl,destfile = dir)
    }
test <- read.csv("testing.csv")
```

# Cleaning and Organizing the data
The data consists of 6 participants, each was instructed to perform barbell lift at the 5 different levels of performance (A-E). The variable 'new_window' denotes the switch between 2.5 s time-windows. The authors summarized the variables for each 2.5 s time-window. I will exclude these summary variables and include only raw data.
```{r warning=FALSE,message=FALSE}
nadat <- apply(train,2,function(x) sum(is.na(x))/length(x)) ## percentage of missing values per column
col.exclude <- names(nadat[nadat > 0.1]) ## columns of the summary variables created by the authors

no.col <- c("X","new_window","num_window","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp")
full.dat <- train %>% select(-one_of(col.exclude),-one_of(no.col)) ## the full raw data w/o the summary variables
nofacdat <- full.dat[, !sapply(full.dat, is.factor)] ## remove all factor variables
full.dat <- data.frame(nofacdat,classe = full.dat$classe)

## Create data partitioning into training_Set and testing_Set:
set.seed(645)
inTrain = createDataPartition(full.dat$classe, p = 0.7,list = FALSE)

training = full.dat[ inTrain,] ## the training data frame

testing = full.dat[-inTrain,] ## the testing data frame
```

# Choosing a model
I will try several different approaches and will choose the model with the lowest out-of-sample error

## Decision Tree Approach
```{r warning=FALSE,message=FALSE,fig.height=9}
tree_model <- train(classe~.,data = training,method = "rpart")
tree_pred <- predict(tree_model,testing)
cm <- confusionMatrix(tree_pred,testing$classe)
tit <- paste("Decision Tree\nOverall accuracy = ",round(cm$overall[1],3))
fancyRpartPlot(tree_model$finalModel,main = tit)
tree_accuracy <- round(cm$overall[1],3)
```

## Random Forest Approach
```{r warning=FALSE,message=FALSE}
ctrl_rf <- trainControl(method="cv", number=3, verboseIter=FALSE)
rf_model <- train(classe~.,data = training,method = "rf",trControl = ctrl_rf)
rf_pred <- predict(rf_model,testing)
cm <- confusionMatrix(rf_pred,testing$classe)

input.matrix <- data.matrix(cm$table)
input.matrix.normalized <- (input.matrix)/sum(input.matrix)

colnames(input.matrix.normalized) = c("A", "B", "C", "D", "E")
rownames(input.matrix.normalized) = colnames(input.matrix.normalized)

confusion <- as.data.frame(as.table(input.matrix.normalized))

rf_accuracy <- round(cm$overall[1],3)

tit <- paste("Random Forest\nConfusion Matrix\nOverall Accuracy =",rf_accuracy)

plot <- ggplot(confusion)
plot + geom_tile(aes(x=Prediction, y=Reference, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + scale_fill_gradientn(breaks=seq(from=0, to=0.3, length.out = 10),colors = c('#f7f7f7','#fff7f3','#fde0dd','#fcc5c0','#fa9fb5','#f768a1','#dd3497','#ae017e','#7a0177','#49006a'),labels = round(seq(from=0, to=0.3, length.out = 10),2)) + labs(fill="Accuracy") +ggtitle(tit)
```

## Generalized Boosted Regression Approach
```{r warning=FALSE,message=FALSE}
ctrlgbm <- trainControl(method = "repeatedcv",number = 5,repeats = 1)
gbm_model <- train(classe ~ ., data=training, method = "gbm",trControl = ctrlgbm,verbose = FALSE)

gbm_pred <- predict(gbm_model, newdata=testing)
cm <- confusionMatrix(gbm_pred, testing$classe)
gbm_accuracy <- round(cm$overall[1],3)
plot(gbm_model,pch=19,cex = 1.2,lwd = 2)
```

# Conclusions
It seems that the highest accuracy reached with the 'Random Forest' Approach:

1. Decision tree accuracy is: `r tree_accuracy`
2. Random Forest accuracy is: `r rf_accuracy`
3. Generalized Boosting Regression accuracy is: `r gbm_accuracy`

# Predicting the class of the test dataset
I will now use the best model: "Random Forest" to predict the 20 cases given in the test dataset
```{r warning=FALSE,message=FALSE}
predict(rf_model,newdata = test)
```